{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0374af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.model_selection import ShuffleSplit, GridSearchCV, train_test_split\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c018abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos los datos\n",
    "\n",
    "X=np.load(\"./datos_mfcc_pca99_X.npy\")\n",
    "y=np.load(\"./datos_mfcc_pca99_y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d6496e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducimos el conjunto de entrenamiento al 50% usando un método no supervisado. En este caso, un C-means con el que pretendemos\n",
    "# quedarnos con los centros de los clusters que aparezcan, que serán los puntos más representativos.\n",
    "\n",
    "c = 6             # número de clases\n",
    "nc = 120          # muestras por clase\n",
    "n= 720           # número de muestras totales\n",
    "\n",
    "X_reduc=[]\n",
    "y_reduc=np.int16(np.kron(np.arange(c),np.ones(nc//2)))\n",
    "\n",
    "for i in range(0,n,nc):\n",
    "    k_medoids=KMedoids(n_clusters=nc//2)\n",
    "    k_medoids.fit(X[i:(i+nc)])\n",
    "    centros=k_medoids.cluster_centers_\n",
    "    X_reduc.append(centros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ee0f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 464)\n",
      "(360, 464)\n"
     ]
    }
   ],
   "source": [
    "print(X_reduc[0].shape)\n",
    "# Reshapeamos las dimensiones de X_reduc para tenerlo todo en una matriz dónde cada fila será una muestra y cada columna una \n",
    "# característica. La matriz estará ordenada por clases. \n",
    "# Para reshapear, antes habrá que convertir a array.\n",
    "\n",
    "\n",
    "X_reduc=np.array(X_reduc).reshape((360,464))\n",
    "print(X_reduc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93bfc1c",
   "metadata": {},
   "source": [
    "Una vez tenemos el conjunto reducido al 50%, podemos hacer la división en entrenamiento y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "884a0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora ya podemos dividir nuestro conjunto reducido en entrenamiento y test, y probar algunos métodos Kernel.\n",
    "\n",
    "Xtr, Xts, ytr, yts = train_test_split(X_reduc, y_reduc, test_size=0.3, shuffle=True, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a35ddafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(random_state=0)\n",
      "OA train 1.00\n",
      "OA test 0.40\n"
     ]
    }
   ],
   "source": [
    "# Ahora podemos aplicar un algoritmo de tipo red neuronal a nuestros datos. Empezaremos entrenándolo con los parámetros por \n",
    "# defecto.\n",
    "\n",
    "clf_MLP = MLPClassifier(random_state=0)     # Definimos un modelo de clasificación de una red neuronal, con todos los parámetros \n",
    "                                        # por defecto.\n",
    "clf_MLP.fit(Xtr, ytr)       # Ajustamos la red con nuestros datos.\n",
    "print(clf_MLP)\n",
    "print('OA train %0.2f' % clf_MLP.score(Xtr, ytr))  # el método score calcula el error, en este caso en entrenamiento.\n",
    "print('OA test %0.2f' % clf_MLP.score(Xts, yts))    # igual pero en test.\n",
    "\n",
    "#La frontera de decisión sería la línea blanca, justo dónde ocurre el cambio de color.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf39a0c",
   "metadata": {},
   "source": [
    "Parece un claro caso de sobre entrenamiento. Puede deberse a que los parámetros de la red neuronal no son los adecuados. Echamos un vistazo a los parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d6723f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (100,),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.001,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 200,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': 0,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': False,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_MLP.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb071299",
   "metadata": {},
   "source": [
    "En efecto, vemos que uno de los parámetros son las capas ocultas, y está puesto por defecto a 100 capas. Esto evidentemente es demasiado para nuestro problema. En principio con un par de capas ocultas debería de bastar para resolverlo.\n",
    "\n",
    "Sin embargo, vamos a probar con otros parámetros para ver si mejora algo la cosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2529f690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(alpha=1.0, hidden_layer_sizes=(14, 5), random_state=0)\n",
      "OA train 0.62\n",
      "OA test 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Master\\anaconda3\\envs\\MCD_21_22\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento MLP (parámetros fijados)\n",
    "#probamos otras opciones del clasificador buscando que el ajuste sea mejor.\n",
    "\n",
    "clf_MLP2 = MLPClassifier(solver='lbfgs', alpha=1e-0, hidden_layer_sizes=(14, 5), random_state=0)  \n",
    "clf_MLP2.fit(Xtr, ytr)\n",
    "\n",
    "print(clf_MLP2)\n",
    "print('OA train %0.2f' % clf_MLP2.score(Xtr, ytr)) \n",
    "print('OA test %0.2f' % clf_MLP2.score(Xts, yts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b77c6",
   "metadata": {},
   "source": [
    "Obsevamos que no es un buen ajuste, pero al menos no tenemos un caso de overfitting. Al modificar los parámetros cambia la manera de entrenar el clasificador. \n",
    "\n",
    "Lo ideal es hacer una búsqueda de los parámetros ideales por cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e635429f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Master\\anaconda3\\envs\\MCD_21_22\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-3, 3, 4)   # 5 = número de combinaciones. Nos interesa mantenerlo pequeño.\n",
    "neurons = [[i,j] for i in range(5,12,3) for j in range(2,6,2)]\n",
    "tuned_parameters = {'solver': ['lbfgs'], 'alpha': alphas, \n",
    "                    'hidden_layer_sizes': neurons,'random_state': [0]}\n",
    "\n",
    "#GridSearchCV nos permite hacer cross validation de forma automatica\n",
    "# Usaremos el cross validation para optimizar los pesos. Haremos varios clasificadores de redes neuronales, y nos quedaremos con \n",
    "# el que nos dé los mejores pesos.\n",
    "\n",
    "clf_MLP3 = GridSearchCV(MLPClassifier(solver='lbfgs'), tuned_parameters, cv=5,n_jobs=-1,verbose=2)\n",
    "clf_MLP3.fit(Xtr,ytr)  # Con este fit, estamos ajustando todas las combinaciones obtenidas del cross validation.\n",
    "clf_MLP3=clf_MLP3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0facceed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(464, 11), (11, 4), (4, 6)]\n",
      "MLPClassifier(alpha=10.0, hidden_layer_sizes=[11, 4], random_state=0,\n",
      "              solver='lbfgs')\n",
      "OA train 0.99\n",
      "OA test 0.50\n"
     ]
    }
   ],
   "source": [
    "#classifier.coefs_ contains the weight matrices that constitute the model parameters:\n",
    "print([coef.shape for coef in clf_MLP3.coefs_])  \n",
    "\n",
    "print(clf_MLP3)\n",
    "print('OA train %0.2f' % clf_MLP3.score(Xtr, ytr)) \n",
    "print('OA test %0.2f' % clf_MLP3.score(Xts, yts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9a2de9",
   "metadata": {},
   "source": [
    "Parece un caso de overfitting de nuevo. Los parámetros óptimos no parecen ser buenos tampoco. \n",
    "\n",
    "Quizás lo que ocurre es que tenemos pocos datos de entrenamiento. Las redes neuronales necesitan un conjunto de entrenamiento grande para tener un buen rendimiento. Por lo tanto, quizás este método no es el adecuado para resolver nuestro problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79c5cecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  2  1  1  3  3]\n",
      " [ 0 15  0  0  2  3]\n",
      " [ 1  5  4  7  2  1]\n",
      " [ 0  3  2 16  1  0]\n",
      " [ 3  4  0  2  8  1]\n",
      " [ 3  2  0  0  2  4]]\n"
     ]
    }
   ],
   "source": [
    "preds_test = clf_MLP3.predict(Xts)\n",
    "CM_MLP=metrics.confusion_matrix(yts,preds_test)\n",
    "print(CM_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe71c16a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
